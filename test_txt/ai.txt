# separator
Transformer 架构通过自注意力机制（self-attention）取代了传统的循环结构，在自然语言处理任务中显著提升了并行性和长距离依赖建模能力。

# separator
大语言模型（LLM）通常基于海量文本预训练，再通过指令微调或人类反馈强化学习（RLHF）对齐用户意图。

# separator
在监督学习中，模型通过最小化预测输出与真实标签之间的损失函数（如交叉熵）来优化参数。

# separator
生成对抗网络（GAN）由生成器和判别器组成，二者通过对抗博弈提升生成样本的真实感，常用于图像合成。

# separator
AI 对齐（AI alignment）旨在确保人工智能系统的行为符合人类价值观和预期目标，是安全研究的关键方向。

# separator
少样本学习（few-shot learning）使模型能在仅提供少量示例的情况下泛化到新任务，常借助提示工程或元学习实现。

# separator
嵌入（embedding）是将离散符号（如词或实体）映射为连续向量的技术，保留语义相似性，广泛应用于检索和推荐系统。

# separator
推理阶段的模型量化（quantization）可将浮点权重转为低比特整数，显著降低内存占用和计算延迟，适用于边缘设备部署。

# separator
多模态模型（如 CLIP 或 LLaVA）能够联合处理文本、图像等不同模态信息，实现跨模态理解与生成。

# separator
幻觉（hallucination）指大模型生成看似合理但事实错误的内容，是当前生成式 AI 可靠性面临的主要挑战之一。